{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 498 Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import ssl\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import little_mallet_wrapper\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\uyvie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\uyvie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\uyvie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\uyvie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\uyvie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('./chatgpt.csv.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375027, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>username</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-11 17:18:29+00:00</td>\n",
       "      <td>1634604721058004993</td>\n",
       "      <td>I have just published ChatGptNet, a library th...</td>\n",
       "      <td>marcominerva</td>\n",
       "      <td>129.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-27 11:13:40+00:00</td>\n",
       "      <td>1618930234820288513</td>\n",
       "      <td>How to make money with Chat GPT?\\n\\nFollow the...</td>\n",
       "      <td>Fitness_Empire0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-24 02:19:12+00:00</td>\n",
       "      <td>1639089452545875970</td>\n",
       "      <td>Lack of safeguards for freedom of speech: The ...</td>\n",
       "      <td>rajeshsersia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-01 08:36:15+00:00</td>\n",
       "      <td>1620702557411901441</td>\n",
       "      <td>#ChatGPT: a threat or an opportunity for #educ...</td>\n",
       "      <td>LLInC_Leiden</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-28 10:07:29+00:00</td>\n",
       "      <td>1619275965078839297</td>\n",
       "      <td>Back in 2020, people thought lock down is fore...</td>\n",
       "      <td>vspeeeeee</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date                   id  \\\n",
       "0  2023-03-11 17:18:29+00:00  1634604721058004993   \n",
       "1  2023-01-27 11:13:40+00:00  1618930234820288513   \n",
       "2  2023-03-24 02:19:12+00:00  1639089452545875970   \n",
       "3  2023-02-01 08:36:15+00:00  1620702557411901441   \n",
       "4  2023-01-28 10:07:29+00:00  1619275965078839297   \n",
       "\n",
       "                                             content         username  \\\n",
       "0  I have just published ChatGptNet, a library th...     marcominerva   \n",
       "1  How to make money with Chat GPT?\\n\\nFollow the...  Fitness_Empire0   \n",
       "2  Lack of safeguards for freedom of speech: The ...     rajeshsersia   \n",
       "3  #ChatGPT: a threat or an opportunity for #educ...     LLInC_Leiden   \n",
       "4  Back in 2020, people thought lock down is fore...        vspeeeeee   \n",
       "\n",
       "   like_count  retweet_count  \n",
       "0       129.0           37.0  \n",
       "1        76.0            9.0  \n",
       "2         0.0            0.0  \n",
       "3         1.0            0.0  \n",
       "4         0.0            0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of rows and columns in data\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "date              0\n",
      "id                4\n",
      "content           4\n",
      "username         28\n",
      "like_count       50\n",
      "retweet_count    50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print('Missing values:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "date              object\n",
      "id                object\n",
      "content           object\n",
      "username          object\n",
      "like_count       float64\n",
      "retweet_count    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print('Data types:')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics:\n",
      "          like_count  retweet_count\n",
      "count  374977.000000  374977.000000\n",
      "mean        7.143417       1.504042\n",
      "std       224.018703      48.334174\n",
      "min         0.000000       0.000000\n",
      "25%         0.000000       0.000000\n",
      "50%         1.000000       0.000000\n",
      "75%         2.000000       0.000000\n",
      "max     64094.000000   16080.000000\n"
     ]
    }
   ],
   "source": [
    "# Show summary of data\n",
    "print('Summary statistics:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords and punctuation\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "# Define a function to perform tokenization with checks for special characters and stop words\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(str(text))\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the text data in the \"content\" column\n",
    "df['content_tokenized'] = df['content'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner(tokenized_text):\n",
    "    \n",
    "    # Perform part-of-speech tagging, label nouns, verbs, adjectives, etc. \n",
    "    tagged_text = nltk.pos_tag(tokenized_text)\n",
    "    \n",
    "    # Perform named entity recognition, perform named entity recognition\n",
    "    ner_result = ne_chunk(tagged_text)\n",
    "    return ner_result\n",
    "\n",
    "# Apply NER to the 'content_tokenized' column\n",
    "df['ner_result'] = df['content_tokenized'].apply(perform_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the named entity results and extract named entities\n",
    "flattened_entities = []\n",
    "\n",
    "# Extract named entities\n",
    "for sublist in df['ner_result']:\n",
    "    for ent in sublist:\n",
    "        if isinstance(ent, Tree):\n",
    "            flattened_entities.append(' '.join([token[0] for token in ent.leaves()]))\n",
    "\n",
    "# Count the frequency of named entities\n",
    "entity_counts = Counter(flattened_entities)\n",
    "\n",
    "# Set the number of top named entities to display\n",
    "top_n = 10\n",
    "\n",
    "# Get the top N named entities\n",
    "top_entities = entity_counts.most_common(top_n)\n",
    "\n",
    "# Display the top N named entities\n",
    "for entity, count in top_entities:\n",
    "    print(entity, count)\n",
    "\n",
    "# Extract entities and counts for plotting\n",
    "entities = [entity for entity, count in top_entities]\n",
    "counts = [count for entity, count in top_entities]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.bar(range(len(entities)), counts, align='center')\n",
    "plt.xticks(range(len(entities)), entities, rotation=45)\n",
    "plt.xlabel('Named Entities')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Top {top_n} Named Entities')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the percentage threshold for a given tweet, this percentage cuts off tweets that have \n",
    "# under X% of tokens/words that are present in the vader lexicon and correctly processed\n",
    "\n",
    "# Specify the desired threshold (e.g., 20%)\n",
    "threshold = 0.35\n",
    "\n",
    "# Index of specific tweet to analyze, this is mainly used for debugging but it is interesting to see how the tokenization works \n",
    "tweet_index = 50\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Get the list of stopwords and punctuation\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to perform tokenization with checks for special characters, stop words, and VADER lexicon presence\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
    "\n",
    "    #Remove tokens starting with a special character\n",
    "    tokens = [token for token in tokens if not re.match(r'^[^a-zA-Z0-9]', token)]\n",
    "\n",
    "    # Remove URLs and links\n",
    "    tokens = [token for token in tokens if not re.match(r'^https?://[^\\s]+', token)]\n",
    "\n",
    "    # Remove trailing special characters\n",
    "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords_list and token in sid.lexicon]\n",
    "    filtered_tokens = [token for token in filtered_tokens if token not in punctuation_set]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Define a function to print words and their sentiment scores\n",
    "def print_word_sentiment(tokens):\n",
    "    for token in tokens:\n",
    "        scores = sid.polarity_scores(token)\n",
    "        print(f'Word: {token}\\tSentiment Scores: {scores}')\n",
    "\n",
    "# Define a function to calculate the average sentiment score for a tweet\n",
    "def calculate_average_score(tokens):\n",
    "    scores = [sid.polarity_scores(token)['compound'] for token in tokens]\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "# Define a function to check if the given percentage of words are successfully tokenized and calculate the tokenization percentage\n",
    "def check_tokenization_threshold(tokens, threshold):\n",
    "    num_tokens = len(tokens)\n",
    "    if num_tokens == 0:\n",
    "        return False, 0.0\n",
    "    filtered_tokens = filter_tokens(tokens)\n",
    "    num_successful_tokens = len(filtered_tokens)\n",
    "    tokenization_percentage = num_successful_tokens / num_tokens\n",
    "    return tokenization_percentage >= threshold, tokenization_percentage\n",
    "\n",
    "df['content_tokenized'] = df['content'].apply(tokenize_text)\n",
    "df['tokenization_percentage'] = df['content_tokenized'].apply(lambda tokens: check_tokenization_threshold(tokens, threshold)[1])\n",
    "\n",
    "# Filter out texts that don't meet the tokenization threshold\n",
    "df = df[df['tokenization_percentage'] >= threshold]\n",
    "\n",
    "# Example usage to select and analyze a specific tweet by index\n",
    "selected_tweet = df['content'].iloc[tweet_index]\n",
    "tokenized_tweet = df['content_tokenized'].iloc[tweet_index]\n",
    "\n",
    "if tokenized_tweet:\n",
    "    print(f'Selected Tweet: {selected_tweet}')\n",
    "    print('\\nTokenized Content:')\n",
    "    print(tokenized_tweet)\n",
    "\n",
    "    print('\\nIndividual Word Sentiment Scores:')\n",
    "    print_word_sentiment(tokenized_tweet)\n",
    "\n",
    "    average_score = calculate_average_score(tokenized_tweet)\n",
    "    print(f'\\nAverage Score: {average_score}')\n",
    "else:\n",
    "    print('Text does not meet the tokenization threshold and cannot be analyzed.')\n",
    "\n",
    "# Print the updated DataFrame with the tokenization percentage column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average sentiment score for each tweet\n",
    "df['average_sentiment'] = df['content_tokenized'].apply(calculate_average_score)\n",
    "\n",
    "# Create a distribution plot of the average sentiment scores\n",
    "plt.hist(df['average_sentiment'], bins=10, edgecolor='black')\n",
    "plt.xlabel('Average Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Average Sentiment Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./chatgpt.csv.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374977, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>username</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-11 17:18:29+00:00</td>\n",
       "      <td>1634604721058004993</td>\n",
       "      <td>I have just published ChatGptNet, a library th...</td>\n",
       "      <td>marcominerva</td>\n",
       "      <td>129.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-27 11:13:40+00:00</td>\n",
       "      <td>1618930234820288513</td>\n",
       "      <td>How to make money with Chat GPT?\\n\\nFollow the...</td>\n",
       "      <td>Fitness_Empire0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-24 02:19:12+00:00</td>\n",
       "      <td>1639089452545875970</td>\n",
       "      <td>Lack of safeguards for freedom of speech: The ...</td>\n",
       "      <td>rajeshsersia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-01 08:36:15+00:00</td>\n",
       "      <td>1620702557411901441</td>\n",
       "      <td>#ChatGPT: a threat or an opportunity for #educ...</td>\n",
       "      <td>LLInC_Leiden</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-28 10:07:29+00:00</td>\n",
       "      <td>1619275965078839297</td>\n",
       "      <td>Back in 2020, people thought lock down is fore...</td>\n",
       "      <td>vspeeeeee</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date                   id  \\\n",
       "0  2023-03-11 17:18:29+00:00  1634604721058004993   \n",
       "1  2023-01-27 11:13:40+00:00  1618930234820288513   \n",
       "2  2023-03-24 02:19:12+00:00  1639089452545875970   \n",
       "3  2023-02-01 08:36:15+00:00  1620702557411901441   \n",
       "4  2023-01-28 10:07:29+00:00  1619275965078839297   \n",
       "\n",
       "                                             content         username  \\\n",
       "0  I have just published ChatGptNet, a library th...     marcominerva   \n",
       "1  How to make money with Chat GPT?\\n\\nFollow the...  Fitness_Empire0   \n",
       "2  Lack of safeguards for freedom of speech: The ...     rajeshsersia   \n",
       "3  #ChatGPT: a threat or an opportunity for #educ...     LLInC_Leiden   \n",
       "4  Back in 2020, people thought lock down is fore...        vspeeeeee   \n",
       "\n",
       "   like_count  retweet_count  \n",
       "0       129.0           37.0  \n",
       "1        76.0            9.0  \n",
       "2         0.0            0.0  \n",
       "3         1.0            0.0  \n",
       "4         0.0            0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"content\" column to string data type\n",
    "df['content'] = df['content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a sample of the dataset to ensure reasonable run time\n",
    "work_df = df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all the Twitter posts to lowercase and remove stopwords, punctuation, and numbers\n",
    "training_data = [little_mallet_wrapper.process_string(text, numbers='remove') for text in work_df['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the original (not pre-processed) Twitter posts\n",
    "original_texts = []\n",
    "\n",
    "for text in work_df['content']:\n",
    "    original_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Mallet\n",
    "\n",
    "# Number of topics to be returned\n",
    "num_topics = 20\n",
    "\n",
    "path_to_mallet = '../mallet/bin/mallet'\n",
    "\n",
    "training_data = training_data\n",
    "\n",
    "# Set output directory\n",
    "output_directory_path = 'topic-model-output/'\n",
    "\n",
    "# Create output directory\n",
    "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create output files\n",
    "path_to_training_data           = f'{output_directory_path}/training.txt'\n",
    "path_to_formatted_training_data = f'{output_directory_path}/mallet.training'\n",
    "path_to_model                   = f'{output_directory_path}/mallet.model.{str(num_topics)}'\n",
    "path_to_topic_keys              = f'{output_directory_path}/mallet.topic_keys.{str(num_topics)}'\n",
    "path_to_topic_distributions     = f'{output_directory_path}/mallet.topic_distributions.{str(num_topics)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Complete\n",
      "Training topic model...\n",
      "Complete\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "# Train topic model\n",
    "little_mallet_wrapper.quick_train_topic_model(path_to_mallet,\n",
    "                                              output_directory_path,\n",
    "                                              num_topics,\n",
    "                                              training_data)\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú®Topic 0‚ú®\n",
      "\n",
      "['chatgpt', 'gpt', 'chat', 'like', 'use', 'think', 'one', 'know', 'time', 'people', 'using', 'good', 'would', 'make', 'get', 'better', 'even', 'could', 'see', 'need']\n",
      "\n",
      "‚ú®Topic 1‚ú®\n",
      "\n",
      "['chatgpt', 'https', 'openai', 'users', 'plus', 'million', 'get', 'free', 'month', 'app', 'paid', 'per', 'months', 'subscription', 'access', 'number', 'chatgptplus', 'daily', 'days', 'version']\n",
      "\n",
      "‚ú®Topic 2‚ú®\n",
      "\n",
      "['https', 'chatgpt', 'python', 'cybersecurity', 'trading', 'machinelearning', 'stocks', 'daysofcode', 'options', 'news', 'tech', 'deeplearning', 'free', 'fintech', 'iot', 'web', 'take', 'bitcoin', 'investing', 'investments']\n",
      "\n",
      "‚ú®Topic 3‚ú®\n",
      "\n",
      "['https', 'chatgpt', 'crypto', 'gpt', 'nft', 'airdrop', 'bitcoin', 'web', 'blockchain', 'btc', 'eth', 'cryptocurrency', 'magic', 'imgnai', 'token', 'future', 'nfts', 'ape', 'ethereum', 'powerful']\n",
      "\n",
      "‚ú®Topic 4‚ú®\n",
      "\n",
      "['chatgpt', 'language', 'https', 'model', 'gpt', 'data', 'openai', 'text', 'models', 'based', 'new', 'large', 'trained', 'api', 'generative', 'like', 'natural', 'used', 'tasks', 'tool']\n",
      "\n",
      "‚ú®Topic 5‚ú®\n",
      "\n",
      "['https', 'chatgpt', 'midjourney', 'aiart', 'dalle', 'art', 'openai', 'stablediffusion', 'prompt', 'join', 'artificialintelligence', 'imagine', 'nft', 'gpt', 'diffusion', 'stable', 'digitalart', 'co/rlyimpqw', 'dall', 'images']\n",
      "\n",
      "‚ú®Topic 6‚ú®\n",
      "\n",
      "['https', 'chatgpt', 'seo', 'artificialintelligence', 'marketing', 'via', 'trending', 'prompts', 'chatgptprompts', 'chatbot', 'content', 'views', 'workfromhome', 'aichatbot', 'makemoneyonline', 'jasperai', 'chatbotai', 'sidehustle', 'aiprompts', 'openai']\n",
      "\n",
      "‚ú®Topic 7‚ú®\n",
      "\n",
      "['https', 'chatgpt', 'coinex', 'top', 'become', 'feb', 'congratulations', 'gainer', 'coinexcom', 'agix', 'professionals', 'tag', 'thank', 'march', 'est', 'confused', 'elt', 'main', 'join', 'vtuber']\n",
      "\n",
      "‚ú®Topic 8‚ú®\n",
      "\n",
      "['chatgpt', 'https', 'asked', 'answer', 'question', 'response', 'questions', 'story', 'first', 'word', 'got', 'wrong', 'gave', 'okay', 'comment', 'one', 'children', 'correct', 'list', 'words']\n",
      "\n",
      "‚ú®Topic 9‚ú®\n",
      "\n",
      "['chat', 'gpt', 'https', 'elonmusk', 'twitter', 'elon', 'woke', 'dan', 'musk', 'left', 'programmed', 'tell', 'bias', 'asked', 'knows', 'tweet', 'tesla', 'guy', 'thinks', 'know']\n",
      "\n",
      "‚ú®Topic 10‚ú®\n",
      "\n",
      "['chatgpt', 'https', 'technology', 'artificialintelligence', 'openai', 'future', 'tech', 'new', 'read', 'business', 'potential', 'generativeai', 'article', 'latest', 'world', 'learn', 'check', 'chatbot', 'innovation', 'impact']\n",
      "\n",
      "‚ú®Topic 11‚ú®\n",
      "\n",
      "['https', 'chatgpt', 'write', 'asked', 'poem', 'wrote', 'exam', 'pass', 'style', 'written', 'music', 'love', 'poetry', 'song', 'letter', 'cover', 'poems', 'bar', 'upsc', 'day']\n",
      "\n",
      "‚ú®Topic 12‚ú®\n",
      "\n",
      "['chatgpt', 'https', 'content', 'use', 'using', 'create', 'write', 'video', 'prompts', 'code', 'help', 'writing', 'see', 'game', 'get', 'new', 'tools', 'free', 'check', 'learn']\n",
      "\n",
      "‚ú®Topic 13‚ú®\n",
      "\n",
      "['gpt', 'chat', 'https', 'ask', 'use', 'write', 'lol', 'get', 'youtube', 'using', 'bot', 'love', 'money', 'make', 'shit', 'via', 'open', 'says', 'used', 'elonmusk']\n",
      "\n",
      "‚ú®Topic 14‚ú®\n",
      "\n",
      "['chatgpt', 'https', 'google', 'openai', 'microsoft', 'bing', 'search', 'bard', 'chatbot', 'new', 'tech', 'technology', 'powered', 'engine', 'artificialintelligence', 'launch', 'market', 'news', 'company', 'bardai']\n",
      "\n",
      "‚ú®Topic 15‚ú®\n",
      "\n",
      "['intelligence', 'artificial', 'https', 'chatgpt', 'openai', 'technology', 'human', 'artificialintelligence', 'robots', 'world', 'business', 'like', 'innovation', 'machinelearning', 'programming', 'replace', 'robotics', 'ceo', 'robot', 'jobs']\n",
      "\n",
      "‚ú®Topic 16‚ú®\n",
      "\n",
      "['control', 'https', 'legal', 'power', 'systems', 'data', 'public', 'energy', 'computing', 'social', 'issues', 'care', 'net', 'pros', 'national', 'health', 'security', 'needed', 'ethical', 'major']\n",
      "\n",
      "‚ú®Topic 17‚ú®\n",
      "\n",
      "['chatgpt', 'https', 'students', 'education', 'learning', 'student', 'writing', 'teaching', 'teachers', 'use', 'school', 'research', 'academic', 'papers', 'edtech', 'university', 'using', 'educators', 'schools', 'exams']\n",
      "\n",
      "‚ú®Topic 18‚ú®\n",
      "\n",
      "['https', 'chatgpt', 'domains', 'domain', 'sale', 'openai', 'web', 'tech', 'artificialintelligence', 'betamoroney', 'machinelearning', 'nft', 'spirosmargaris', 'metaverse', 'sunset', 'sunrise', 'domainnames', 'current', 'nicochan', 'domainforsale']\n",
      "\n",
      "‚ú®Topic 19‚ú®\n",
      "\n",
      "['chatgpt', 'https', 'short', 'time', 'price', 'long', 'last', 'rsi', 'powered', 'recommendation', 'sell', 'ticker', 'wait', 'gonna', 'spot', 'binance', 'min', 'wall', 'stories', 'interval']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all the topics with topic number and topic key words\n",
    "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
    "\n",
    "for number, topic in enumerate(topics):\n",
    "    print(f\"‚ú®Topic {number}‚ú®\\n\\n{topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the topic distributions for all the documents (the probability that each document contains each of the topics)\n",
    "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that will help to examine the top Twitter posts for each topic\n",
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def make_md(string):\n",
    "    \"\"\"A function that transforms string data into Markdown\n",
    "    so it can be nicely formatted with bolding and emojis\n",
    "    \"\"\"\n",
    "    display(Markdown(str(string)))\n",
    "\n",
    "def get_top_docs(docs, topic_distributions, topic_index=1, n=5):\n",
    "    \n",
    "    \"\"\"A function that shows the top documents for a given set of topic distributions\n",
    "    and a specific topic number\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_data = sorted([(_distribution[topic_index], _document) for _distribution, _document in zip(topic_distributions, docs)], reverse=True)\n",
    "    topic_words = topics[topic_index]\n",
    "    make_md(f\"### ‚ú®Topic {topic_index}‚ú®\\n\\n{topic_words}\\n\\n---\")\n",
    "    \n",
    "    for probability, doc in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        for word in topic_words:\n",
    "            if word in doc.lower():\n",
    "                doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc, re.IGNORECASE)\n",
    "        make_md(f'‚ú®  \\n**Topic Probability**: {probability}  \\n**Document**: {doc}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ‚ú®Topic 1‚ú®\n",
       "\n",
       "['chatgpt', 'https', 'openai', 'users', 'plus', 'million', 'get', 'free', 'month', 'app', 'paid', 'per', 'months', 'subscription', 'access', 'number', 'chatgptplus', 'daily', 'days', 'version']\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9387115241913839  \n",
       "**Document**: Timeline for adoption of 1 **million** **plus** **users**:\n",
       "\n",
       "#Netflix ‚Äì 3 years and 5 **months**\n",
       "#Twitter ‚Äì 2 years\n",
       "#Spotify - 2 years **plus**\n",
       "#Pinterest - 20 **months**\n",
       "#ClassPass - 12 months\n",
       "#Facebook ‚Äì 10 months\n",
       "#Hulu - 10 months\n",
       "#Instagram ‚Äì 2.5 months\n",
       "\n",
       "#ChatGPT ‚Äì 5 **days** \n",
       "\n",
       "Source: @Vitdwesh\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.8367001126248945  \n",
       "**Document**: These tiktok hearings are the trailer to **get** us invested in the @OpenAI #ChatGPT hearings.\n",
       "\n",
       "Those will be GOLD!\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.8354987214622681  \n",
       "**Document**: Time it took to hit 1 Million **users**:\n",
       "\n",
       "#Facebook - 2 years\n",
       "#Instagram - 2 years\n",
       "#Pinterest - 5 **months**\n",
       "#AngryBirds - 34 **days**\n",
       "#ChatGPT - 5 **days**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.8153747432536841  \n",
       "**Document**: Update **version** v.1.0.5, XChatBot ChatGPT Complete Flutter App. Add Webview (InApp Web). **https**://t.co/9OAdUm8vQC #flutter #flutterdev #xchatbot #**chatgpt** #**openai** #OpenAIChatGPT #flutterapp #flutterid **https**://t.co/4kdWK5XZ1s\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.8153747432536841  \n",
       "**Document**: Booking a flight has never been easier! With HappyFares, you **get** the best deals and the most convenient booking experience. üõ´‚úàÔ∏è \n",
       ".\n",
       ".\n",
       ".\n",
       "#bestflightbookingwebsite #happyfares #flyhigh #happyfares #flight #Number1 #travel #travelling #travelgram #travelphotography #ChatGPT **https**://t.co/ukbJWL2mK6\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the top 5 Twitter posts with the highest probability of containing the given topic\n",
    "get_top_docs(work_df['content'], topic_distributions, topic_index=1, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ‚ú®Topic 2‚ú®\n",
       "\n",
       "['https', 'chatgpt', 'python', 'cybersecurity', 'trading', 'machinelearning', 'stocks', 'daysofcode', 'options', 'news', 'tech', 'deeplearning', 'free', 'fintech', 'iot', 'web', 'take', 'bitcoin', 'investing', 'investments']\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9361340915584833  \n",
       "**Document**: $BK Awaiting Buy Signal based off 7 signals $939 net profit 7.41 profit factor 85% win rate on a 15-min chart. #**trading** #**stocks** #**options** #**chatgpt** #ai #contentmassive üöÄ Free trial at **https**://t.co/tX8L2oEUgX **https**://t.co/EsVmsZ8rhZ\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9221372521300545  \n",
       "**Document**: The Ascent of ChatGPT\n",
       "\n",
       "**https**://t.co/Xus4kWb1gF\n",
       "\n",
       "#Python #DataScientist #BigData #Analytics #DataScience #AI #TensorFlow #JavaScript #CloudComputing #Coding #100DaysofCode #programming #flutter #SQL #MLOps #satriaadhipradana #ChatGPT\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9221372521300545  \n",
       "**Document**: Take a look to $YINN\n",
       " #daytrading  #StocksToWatch  #bottomfishing  #**options**  #**news**  #**trading**  #Stocks  #**investing**  #RedditArmy  #YOLO  #FOMO  #**investments**  #StocksToBuy  #ToTheMoon #ChatGPT\n",
       " **https**://t.co/uxpaqv8azG\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9221372521300545  \n",
       "**Document**: Take a look to $KRT\n",
       " #Stocks  #RedditArmy  #bottomfishing  #**trading**  #FOMO  #**news**  #**investing**  #ToTheMoon  #StocksToBuy  #YOLO  #**investments**  #StocksToWatch  #daytrading  #**options** #ChatGPT\n",
       " **https**://t.co/jX6EGvcfld\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9221372521300545  \n",
       "**Document**: Take a look to $GSD\n",
       " #ToTheMoon  #**investing**  #StocksToWatch  #daytrading  #Stocks  #RedditArmy  #bottomfishing  #StocksToBuy  #**investments**  #FOMO  #YOLO  #**trading**  #**news**  #**options** #ChatGPT\n",
       " **https**://t.co/u3Y3nXpWCT\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the top 5 Twitter posts with the highest probability of containing the given topic\n",
    "get_top_docs(work_df['content'], topic_distributions, topic_index=2, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ‚ú®Topic 3‚ú®\n",
       "\n",
       "['https', 'chatgpt', 'crypto', 'gpt', 'nft', 'airdrop', 'bitcoin', 'web', 'blockchain', 'btc', 'eth', 'cryptocurrency', 'magic', 'imgnai', 'token', 'future', 'nfts', 'ape', 'ethereum', 'powerful']\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9361126739825462  \n",
       "**Document**: Just checked my portfolio and this #GPT4 is doing crazy numbers! üî•üî•üî• \n",
       "\n",
       "üëâ**https**://t.co/KixnEguJ4Y\n",
       "\n",
       "  #**airdrop** #NFT #OpenAIChatGPT #ChatGPT #OPTIMUS $ARB $SUI $USDT $BONK  $RINIA $ETH $SHIB $RDNT $EVMOS $FLOKI $ARB  $ARB **https**://t.co/iGEZGrqqg8\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.925381802463898  \n",
       "**Document**: #ChatGPT can triple productivity.ü¶æüß† Are you ready to enter the world of #AI + #Crypto = #GPT4?\n",
       "\n",
       "Uniswap üëâ **https**://t.co/hzmODBSFtw\n",
       "Dextool üëâ **https**://t.co/zuwVBIyrwM\n",
       "\n",
       "#AI $EGGS #AI $GPT4 $SUI $USDT $HEX https://t.co/kjU8CMzMFF\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9221111406825565  \n",
       "**Document**: GPT-4 is 500 Times More **powerful** than the current ChatGPT, so is $GPT4 \n",
       "\n",
       "üëâ**https**://t.co/T3Ks0zNyaj\n",
       "\n",
       " #**airdrop** #NFT $MAGIC $BOO #ChatGPT $hook $**magic** $APE $MATIC **https**://t.co/h0HRlAUfHZ\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9221111406825565  \n",
       "**Document**: GM!‚òïÔ∏è‚òÄÔ∏è \n",
       "Just aped 3 **eth** in #GPT4 and it is pumping!! üìàüìàüìà\n",
       "Have you #HODL #GPT4 before its #ATH ? üöÄüöÄüöÄ Buy the **token** at Uniswap before it pumps TOOO high!!!\n",
       "\n",
       "ü¶Ñ\n",
       "**https**://t.co/PSxNr7xKah\n",
       "\n",
       "#zksync #ChatGPT **https**://t.co/Zl6sIAMMUu\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‚ú®  \n",
       "**Topic Probability**: 0.9185406163096128  \n",
       "**Document**: GPT-4 is 500 Times More **powerful** than the current ChatGPT, so is $GPT4 \n",
       "\n",
       "üëâ**https**://t.co/dfFxkJWplp\n",
       "\n",
       " #**airdrop** #NFT $TSUKA $imgnAI $BONE $ocean $BCB #Damus $APE **https**://t.co/CQTiPS5Cxm\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the top 5 Twitter posts with the highest probability of containing the given topic\n",
    "get_top_docs(work_df['content'], topic_distributions, topic_index=3, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the topic words and the top documents, I would label each topic as the following:\n",
    "1. Topic 1 Label - Rapid Success and Usage of AI\n",
    "2. Topic 2 Label - \"Get-Rich-Quick\" AI Callouts\n",
    "3. Topic 3 Label - AI and Cryptocurrency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
